[model_config]
query_block_conv_in_dim = 2048
query_block_conv_out_dim = 2048
query_block_conv_stride = 1
query_block_conv_padding = 1
query_block_conv_kernel_size= 3
LM = GPT2
groupnorm = 8
LM_block_read_dim = 2048
LM_block_write_dim = 1024

[training_config]
max_epoch = 10
train_batch_size = 16
eval_batch_size = 8
learning_rate = 3e-4
warmup_tokens = 0.1
final_tokens = 2
lr_decay = False
num_workers = 8
ckpt_path = /home/mhmd_sabry_ab/models/GPT2/model_checkpoints/GPT2
intermediate_tokens_saves = 0.5
weight_decay= 0.01
betas_1= 0.90 
betas_2=0.95
TPU = True

[kiba_dataset]
kiba_path = /home/mhmd_sabry_ab/data/kiba/
kiba_train_len = 78836
kiba_val_len =  19709

